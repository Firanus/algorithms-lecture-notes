# 2.4 - Theory of Algorithms

Order of Growth classifications are so important that they've actually led to a huge amount of research in recent times.

## Input Types

One problem is that the types of input can cause the algoirthmic performance to vary.
* The **best** case gives us a lower bound for the performance.
* The **worst** case gives us an upper bound for the performance.
* The **average** case gives us the performance we'd expect of a random input. This lets us predict performance.

Our 3-sum is a case where all these cases have a similar, O(N^3) case.

In a binary search:
* Best case is 1 (find it immediately)
* Average and Worst case are both lgN

To deal with this variance, there are two approaches we can take:
1. Design for the **worst case**, to ensure it always works quickly. This is ideal, though not always feasible.
2. Alternatively, we can **randomise our inputs**, and depend on a probablistic guarantee of performance. (This is when amortized costs come in.)

## Theory of Algorithms

The above considerations lead us to the theory of algorithms:

#### Goals
* Establish the "difficulty" of a problem.
* Develoop "optimal" algorithms.

#### Approach
* Suppress details in analysis: analyze "to within a constant factor".
* Eliminate variability in the input model by focusing on the worst case.

#### Optimal Algorithm
* We want to be able to provide a guarantee of the performance of the algorithm to within a constant factor.
* We also want to show no algorithm can provide a better performance guarantee.

## Notations

And so, finally, we come to our different forms of notation:

* Big Theta - Provides an asymptotic order of growth. We use this to classify algorithms.
* Big O - Provides an upper bound on algorithmic performance.
* Big Omega - Provides a lower bound on algorithmic performance.

Note that in industry, "Big O" is terminology used to encapsulate, essentially big theta.

## Examples

A way to approach these algorithms is to look at the upper and lower bounds. Lower bounds can be established by determining the **best conceivable runtime** (BCR), as talked about in Cracking the Coding Interview. In other words, what's the minimum theoretical amount of work you would need to do to be able to solve the problem in question?

Upper bounds can often be established by simply theorizing about the brute force solution to a problem. We then revise these down towards our lowest bound by investigating other approaches. Cracking the Coding Interview provides an excellent look at approaches for doing this (admittedly, outside an academic setting).

Some examples

#### 1-Sum

Our lower bound is Omega(N), as we have to touch every array entry. A simple brute force is also O(N), so our brute force is demonstrably the optimal algorithm.

#### 3-Sum

Our lower bound is again Omega(N). However, our upper bound is now O(N^2 logN), as we found by using the binary search. We have a gap between upper and lower bounds. In point of fact, we actually don't know what the optimal algorithm is; this is an open problem in research. In point of fact, we don't even know if this problem can be solved by an algorithm of quadratic complexity.

In point of fact, we now live in a golden age of algorithm design.

## Caveats
Is it always best to look at the worst case? In many fields of engineering, this is not the approach.

Also, can we really ignore the constant factor? In many practical cases, it can be important. In this course, we will be using **tilde** notation, which includes the constant factor, and provides a rough upper and lower bound, so that we can more accurately predict performance.