# 1.3 Quick Union Improvements

## Weighting 

One potential improvement to quick union is called weigthing. We essentially avoid making long trees, by always linking the root of the smaller tree below the root of the larger tree. We can do this by keeping track of the size of our trees.

We do this by adding an array size[]

```Java
public class QuickUnionUF
{
  private int[] id;
  private int[] size;
  
  public QuickUnionUF(int N)
  {
    id = new int[N];
    size = new int[N];
    for (int i = 0; i<N; i++)
      id[i] = i;
      size[i] = 1;
  }
  
  public boolean find(int p, int q) 
  {
    return root(p) == root(q)
  }
  
  public void union(int p, int q)
  {
    int rootP = root(p)
    int rootQ = root(q)
    if (rootP == rootQ) return;
    
    //Weighting changes
    if size[rootP] < size[rootQ] {
      id[rootP] = rootQ;
      size[rootQ] = size[rootP];
    } else {
      id[rootQ] = rootP;
      size[rootP] = size[rootQ];
    }
  }
  
  private int root(int p)
  {
    while (id[p] != p) 
    {
      id[p] = id[id[p]] //path compression change
      p = id[p];
    }
    return p;
  }
}
```

### Proof of Node Depth

Now that we have this system in place, we come to an important proposition, namely that the depth of any node is at most lg N (i.e. log_2(N)). How do we prove this?

When does the depth of a node x increase?

* It increases by 1 when the tree T1 containing x is merged into the tree T2.
* The size of the tree containing x at lease doubles, because size(T_2) >= size(T_1). Otherwise we would have merged T2 into T1, not the other way around

The size of the tree containing x can double at most lg(N) times. Why?

* Because if you double 1 lg(N) times, then you get N, which is the total size of the data set.

### Performance Effects

Whereas before, with quick union, we had to worst case complexity of N in the find and union operators, now, the complexity of both operations is lg(N), because the most time that can be spent finding a root is lg(N), and both operations are constant complexity once the root has been found.

This is now acceptable, but we can improve performance still further.

## Path Compression

Just after computing the root of an element p, set the id of each examined node to point at that root.

There are actually two ways we could go about implementing this. First, we cood add a second loop to root to set the id[] of each examined node to the root.

An alternate, one pass variant (slightly less efficient, but simpler) is to make every node we pass point to its grandparent, thereby halving path length.

In practice, there's no reason not to!

### Performace

Proving the complexity of this algorithm is beyond the scope of this course. The result is, for N objects, we would make <= c(N + M lg*(N)) operations, where lg*(X) is the iterated version: number of times you have to perform lg(X) to get 1.

In practice, this number is < 5 as 2^65536 = 5.

We can actually improve this further.

This is close to linear. However, it has been proven that it is not possible to have a linear time algorithm for this problem.

If you compare performances, where Quick Union would have taken 30 years to analyse 1,000,000,000 objects, Weighted Quick Union with Path Compression takes 6 seconds.