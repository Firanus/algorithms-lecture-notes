# 6.2 - Duplicate Keys

Now we'll have a look at what happens when we have a large number of duplicate keys. Often, its a sort's goal to bring together items which have equal keys, so this is not at all an unusual step.

Often in such use cases, we'll have huge files, but a relatively small number of keys.

For **mergesort**, it doesn't really matter how many duplicate keys you have. You can actually show that you'll always have between 1/2 NlgN and NlgN compares.

For **Quicksort** however, the algorithm can actually go quadratic unless you stop partitioning on equal keys. (In the 1990s, a user actually found this defect in C's `qsort()` function).

## The Problem

### The Mistake

The mistake occurs when we put all items equal to the partitioning item on one side. This is because when all the keys are equal our partitioning will sort all the equal keys into one side of the partition, and so our partition essentially does nothing; instead of breaking the array down into roughly equal halves, each partition only peels off one element. This behaviour is identical to the worst case behavious for Quicksort, i.e. ~(1/2)N^2 comparisons

### The Recommended Approach

On the other hand, if we stop the scans when the item is equal to the partitioning item, then even in the 'all elements equal' case, our final result will still divide the elements into two equal arrays (as our `i` and `j` indices will move towards each other at equal pace, and will meet halfway along the array). Thus we still do ~NlgN compares.

### The Desired Approach

However, there's actually a 3rd, desired approach. Why not put all the items that are equal to the partitioning item in-place? After all, we can guarantee their relative positions in the array.

This is what we will work towards.

## 3-Way Partitioning

Our aim here is to divide the array into 3 parts, using 4 indices:
* Entries less than the partitioning element should be contained between indices `lo` and `lt`.
* Entries equal to the partitioning element should be contained between indices `lt` and `gt`.
* Entries greater than the partitioning element should be contained between the indices 'gt' and 'hi'.

Until the 1990s, the conventional wisdom was that this was not woth doing. However, evidence like the discovery of the `qsort()` problem changed that logic.

### Conceptual Process

* Let `v` be the partitioning item `a[lo]`
* Scan `i` from left to right.
  * `(a[i] < v)`: exchange `a[lt]` with `a[i]`; increment both `lt` and `i`
  * `(a[i] > v)`: exchange `a[gt]` with `a[i]`; decrement `gt`
  * `(a[i] == v)`: increment `i`

Terminate when `i <= gt`

### Code
```Java
public static void sort(Comparable[] a, int lo; int hi) {
  if (hi <= lo) return;
  int i = lo, lt = lo, gt = hi;
  Comparable v = a[lo];
  
  while(i <= gt) {
    if      (less(a[i], v)) exch(a, lt++, i++);
    else if (less(v, a[i])) exch(a, gt--, i);
    else                    i++;
  }
  sort(a, lo, lt - 1);
  sort(a, gt + 1, hi);
}
```

In many ways, this version of Quicksort is actually simpler than the standard implementation. We've implemented it in just over 10 lines (plus the two tiny helper functions).

### Analysis

There are also theoretical reasons why this implementation is important. The first is that the lower bound that we discussed previously was actually built around the assumption that the keys were distinct. In the case we're considering here, that assumption isn't valid.

There is a formula we can derive which shows that how the duplication of a key impacts the sorting lower bound. This equation (which we cannot reproduce here. Markdown yo.) actually shows that the number of compares 3-way partitioning uses is equal to the lower bound within a constant factor. We say that this algorithm is **entropy-optimal**. The proof of this is beyond the scope of the course.

The consequence of this is that a randomized quicksort with 3-way partitioning reduces the running time from linearithmic to linear in a broad range of applications.