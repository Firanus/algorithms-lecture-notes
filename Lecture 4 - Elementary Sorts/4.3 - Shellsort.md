# 4.3 - Shellsort

The third sorting mechanism we'll have a look at is the shellsort, which is more complex than our previous two sorts. (I also don't think I've heard of it before.) It is also one of the oldest sorting algorithms, having been invented in 1959.

## Algorithm

The motivation behind the shellsort is that the insertion sort is essentially inefficient because we move entries only one step at a time when in reality we'd have to move them several steps.

We do this using a procedure called **h-sorting**. An h-sorted array is an array of h interleaved sorted sequences. For example, the array below is h-sorted for h = 4, because all the sub-arrays taken using every 4th element are sorted:

L E E A M H L E P S O L T S X R
L-------M-------P-------T------
--E-------H-------S-------S----
----E-------L-------O-------X--
------A-------E-------L-------R

The shell-sort mechanism than h-sorts the array for decreasing sequence of values of h. For example:

Input:   S H E L L S O R T E X A M P L E
13-sort: P H E L L S O R T E X A M S L E
4-sort:  L E E A M H L E P S O L T S X R
1-sort:  A E E E H L L L M O P R S S T X

### H-sorting mechanism

Our first question then, is how do we h-sort an array? The answer is simple, we just use an insertion sort with a stride length of h. Why do we use an insertion sort?
* For big increments => small subarrays, so insertion sort is quick
* For small increments => our array will be nearly in order

The second fact is  made through by a mathematical proposition, namely:

> A g-sorted array remains g-sorted after h-sorting it.
> 
Proving this fact is surprisingly complex, but the fact that its true is what makes shell sort effective.

The next question is what sequence of increments should we sort it in. This is another surprisingly tough question. Options include:
* **Powers of 2** - **NO**. Not comparing odd indices to even ones will the 1-sort has major potential performance issues
* **Powers of 2 minus 1** - Proposed by Shell himself. Does an ok job
* **3x + 1** - Proposed by Knuth. Easy to compute, and does a good job. (x is the previous increment, hence the sequence looks like 1, 4, 13, 40)

However, finding the optimal sequence is a difficult research problem which has still not been resolved. Sedgewick (the lecturer) himself found an effective sequence (which starts 1, 5, 19, 41, 109...) which is hard to beat empirically. However, nobody knows if it is the best option.

Using Knuth's 3x+1 system as our base, the code for the shellsort looks like:

```Java
public class Shell {
  public static void sort(Comparable[] a) {
    int N = a.length;
    
    int h = 1;
    while (h < N/3) h = h*3 + 1;
    
    while (h > 0) {
      for (int i = h; i < N; i++) {
        for (int j = i; j >= h && less(a[j], a[j-h]); j -= h) {
            exch(a, j, j-h);
        }
      }
      
      // h=h/3 also works, as int division drops remainder, but less readable.
      h = (h - 1) / 3;
    }
  }
  
  //less implementation from before
  //exch implementation from before
}
```

We've also implemented this in Swift:

```Swift
// For identical reasons to the insertion sort (our collection must be both
// mutable and bidirectional) we have to extend an array. We use a 3x+1 sequence
extension Array where Element: Comparable {
    func shellSorted() -> Array {
        var elements = self
        
        var h = 1
        while h < (count-1) / 3 {
            h = (h * 3) + 1
        }
        
        while h > 0 {
            for index in indices {
                var activeIndex = index
                while activeIndex >= startIndex + h,
                    elements[activeIndex] < elements[elements.index(activeIndex, offsetBy: -h)] {
                        let compareIndex = elements.index(activeIndex, offsetBy: -h)
                        elements.swapAt(activeIndex, compareIndex)
                        activeIndex = compareIndex
                }
            }
            
            h = (h - 1) / 3
        }
        
        return elements
    }
}
```

## Complexity

The full analysis of shellsort is actually still an open research problem. However, there are a few things we can say definitively about it.

First, its **worst-case** time complexity with 3x+1 increments is O(N^3/2).

In practice however, its time complex is actually much less than that. For almost all cases, it seems to be that its performance is a constant multiple of either N or NlogN steps, but nobody has yet developed an accurate model.

Why do we care? Well:
* this is a simple idea that leads to substantial performance gains. 
* Useful in practics:
  * It is fast unless the array size is truly huge
  * It only takes a tiny amount of code to implement
  * Often used in hardware and embedded systems because it only takes a very small amount of code to implement
* It poses interesting research questions. We still don't know:
  * Its asymptotic growth rate
  * The best sequence of increments
  * The average-case performance

