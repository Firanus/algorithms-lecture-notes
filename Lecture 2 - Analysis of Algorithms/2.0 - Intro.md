# 2.0 - Analysis of Algorithms

In this lecture we'll be developing the scientific basis for how we analyse our algorithms.

The key metric we'll be focusing on today is **running time**.

## Why Analyse Algorithms?

In this course, we'll mainly be analysing algorithms to predict their performance. In general, we can also use this knowledge to compare algorithms, provide guarantees and understand the theoretical basis underlying our work.

In a more practical setting, what we're largely aiming to do with algorithms is avoid performance bugs. It's unfortunately a common problem for clients that they receive poor performance code because the programmer implementing it did not understand its performance charecteristics. Hence the focus on things like Big-O notation, which we'll get to shortly.

## Success Stories

There're numerous success stories showing how improved algorithms made it possible to solve previously unsolvable problems.

An example is the Discrete Fourier Transform, which breaks a waveform of N samples into periodic components. The Brute Force algorithm has N^2 complexity. The FFT (Fast Fourier Transform) algorithm has NlogN complexity, which allowed the development of extensive new technology. Fourier Transforms are now crucial to DVDs, JPEGs, MRIs, Astrophysics and a great number of ther things.

The N-body simulation problem is another example. Again, brute force was N^2, the Barnes-Hut algorithm is NlogN, and this increase in aloowable Ns has enables a great deal of new research.

## The challenge

Ultimately, the question you have to ask is "Will my program be able to solve a large practical input?"

It was Knuth in the 1970s that first suggested using the scientific method (Observe -> Hypothesize -> Predict -> Verify -> Validate) to analyse algorithms.

Of course, we have the additional restrictions of the scientific method as well:

* Experiments must be **repeatable**
* Hypotheses must be **falsifiable**